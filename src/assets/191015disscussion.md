# Understanding Neural Networks Through Deep Visualization

> http://yosinski.com/deepvis

## 201700181053 赵鑫鉴

1. 根据image or video绘制卷积网络中每一层中神经元的激活值。在一个全连接的convnet中，位置是不重要的，因为任意两个神经元间都有权值传递。如此一来流过每一层网络的数据都可以进行可视化。eg. 将conv5的feature map转换成256个单独的13*13灰度图。

2. 文中引入了几种正则化方法，通过random hyper parameter search找到了work的正则化组合。

   paper中定义了一个参数正则化函数Re(x) penalizes images in various way。该网络在imagenet上训练，解决以下优化问题：$x^*=\text{argmax}_x(ai(x)-Re(x))$，GD函数如下：$x \leftarrow re(x+r(\frac{\partial ai}{\partial x}))$。

   在四种不同的正则化下paper里进行了分析：

   L2 delay, Gaussian blur, Clipping with pixels with small norm, Clipping pixels with small norm 这四种正则方法都有益于可视化。四种正则化组合会有更好的效果，为了得到合适的权值与超参数，对300种可能做了random hyper parameter serach并选择出了四个能很好互补的组合。

   这篇paper的贡献在于有望通过可视化对大型神经网络的运行方式不再是完全的“黑盒”

## 201700122057 于志远

1. 根据图像或视频绘制卷积网络中每一层中神经元的激活值。如果在一个全连接的神经网络中，顺序是不重要的，所以这些向量的图没有空间信息。但在卷积神经网络中，对于2D图像，过滤器以2D卷积的方式应用于图像的两个维度。

2. 本论文通过引入了几种正则化方法来对可视化进行优化：$x^*=\text{argmax}(ai(x)-k_\theta(x))$，当图像呈现给神经网络时，它会为某个单元$i$激活$a_i(x)$（i为所有运行单元的索引），$k_\theta(x)$为一个定义的参数化正则函数$k_\theta(x)$，$x^*$为我们所可视化的图像。梯度下降函数：$x\leftarrow r_\theta(x+r(\frac{\partial ai}{\partial x}))$。

3. 下面四种正则化：

   1. L2衰减：$r_\theta(x)=(1-\theta_{dccay})\cdot x$，防止极端像素求导图像（高振幅）
   2. 高斯模糊：$r_\theta(x)=GaussianBlur(x,\theta_{b\_width})$，惩罚高频信息（高频）
   3. 以较小的范数修剪像素：使$r_\theta(x)$计算范数，较小的设为0，来显示主要对象
   4. 修剪贡献小的像素：去除对激活贡献小的像素，提高速度

   可以通过以上4种方法的排列组合，能产生更好的可视化效果。为了获得一组合理的权重，论文使用了随机超参数搜索，确定了四个能很好互补的组合。

## 201700111085 李宜倬

从某个初始输入$x=x_0$开始，计算此输入在某个单元i处引起的激活$a_i(x)$，然后沿着梯度$\partial ai(x)/\partial x$在输入空间中采取步骤，以合成引起单元i越来越高的激活的输入，最终终止于某个输入，此时$x^*$被认为是该单元的首选输入刺激。在输入空间是图像的情况下，可以直接显示$x^*$进行解释。

通过正则化优化进行可视化，将网络的直接输入x视为零中心输入。

查找$x^*=\text{argmax}(ai(x)-k_\theta(x))*$，有个问题是优化图像以引起高激活会产生无法识别的图像，所以要使用L2-正则化可以生成卷积最终层的稍微可辨别的图像。

所以x0采用渐变步骤搜索x*，使用正则化运算符$r_\theta$，而不是$R_\theta$梯度。当步长为$\eta$时，单个步骤为$x=r_\theta(x+\eta\frac{\partial ai}{\partial x})$。

而L2衰减实现$r_\theta(x)=(1-\theta_{decay})\cdot x$（倾向于防止少数极端像素主导示例图）

其他方法还有高斯模糊，以较小的范数修剪像素以及剪裁激活影响较小的像素。

## 201700272072 王耀宇

深层神经网络被认为是黑匣子，内部运作难以理解。利用可视化以便更好地理解每个神经元所学的知识，从而更好地理解它所执行的计算。

但是经过随机的图像处理的结果太差，无法识别，不具有实际价值，尤其是不经过正则化的图像。

为了产生更多可识别的图像，研究人员尝试优化图像：

1. 最大程度激活神经元
2. 具有类似自然处理的样式

除此之外，有个非常有趣的现象：当对图像进行学习时，会自动学习某些信息，即使并不作为目标（利用可视化的方式可以看出），作为基础信息来辅助目标信息的识别或筛选。

再此外，总结而来即为the Deep Visualization Toolbox工具箱提供了论文所述的功能，总结出了一个model可使用。

## 201705111123 孙维玮

1. 这篇论文尝试通过可视化展示CNN在图像识别的原理。
2. 不进行正则化合成图像可以很好骗过DNN，因此DNN的识别分布是远大于实际人类识别分布的，与该论文同年（2015），GoodFellow在ICLR上提出的FGSM与此后的NIPS神经网络攻防目的在于解决该问题。
3. 引入正则/先验可以从合成图像中看到部分识别物特征，说明DNN并不是没有真正提取出特征。用更强的先验获得更好的效果，我认为目的性过强的方法可能偏离了解神经网络的初衷。
4. 多层展示表明多层NN可以提取不同层次特征，随卷积层加深NN具有更大视野
5. 感觉这个问题本质是对抗NN，可视效果不好不是因为先验不足而是模型本身能力不强，采用GAN可能会得到极为清晰准确的合成图（GAN的CNN识别分布更小，能力更强）。

## 201700150214 王玲雅

通过激活DNN中的隐藏神经元合成图像

输入要具有高激活性

基于梯度的优化：

1. 从一个随机图像开始（随机地为每个像素选一个颜色）
2. 然后使用这个图像作为网络的输入来计算在网络中某个神经元引起的激活
3. 然后反向传递，计算当前的激活相对于网络中之前的激活的梯度
4. 反向传递结束时返回梯度，或如何改变每个像素的颜色，以增加神经元的激活（通过给图像的梯度增加一个小的分数实现）
5. 重复以上计算直到得到神经元高度激活的图像。

这些图像最大地激活了神经元，但没有经过自然图像的先验（如正则化）

优化图像：

1. 最大程度地激活神经元
2. 使图像近似于自然图像（没有像素处于极值），这过程中使用的正则化：使用L2正则化和梯度优化会产生更好的图像，但增加了正则化仍会产生难以识别的图像。

## 201700150100 季明珠

没有经过正则化或预处理的可视神经元

1. 随机为每个像素点选取颜色
2. Input->随机颜色的图像->进行向前传递：计算由图像xi造成的神经元激活ai(x)

进行反向传播：计算梯度（根据先前激活的神经元），最后得到梯度和如何改变像素颜色来提高神经元活性：$x=r_\theta(x+\eta\frac{\partial ai}{\partial x})$，直到神经元高度激活

进行弱正则化：进行弱正则化可能可以产生更为清晰的图像，由偶然产生的高激活神经元产生

用更好的正则化/自然图像优先：会产生更为清晰的图像，使神经元被激发

## 201700210021 张圆圆

图像和视觉领域的神经网络是一种对输入图像进行卷积、降维、机器学习等搭建起的进行信息捕捉和结构化处理的数学模型。之所以被称为神经网络，我觉得一方面是因为它的多层处理，另一方面是“图”结构化，并借鉴了生物中“神经元”定义。在尝试使用MATLAB调用2012.AlexNet神经网络中，约分成了20层处理，AlexNet库中已经完成对自然中上千个图像的特征提取，而对每一个新的输入图像就可以用这20层网络每层之间作一个数据处理/映射，通俗理解，也是降维的一种形式；最后一层给到这个图像最终所属类中。

而本文中的DNN（深度神经网络）中，多次提到了正则化和神经元、在我的理解中，“正则化”对应降维，“神经元”对应特征点，这篇论文将神经网络“黑匣子”的内容进行神经元呈现，也就是把每一个类的最后特征提取进行展示，这些特征是每一个类（如火烈鸟）进行多层神经网络处理后留下的神经元/特征信息。有了这些信息，对新的图像进行预测等就会有一个不错的效果。

正则化的目的在于可以降维泛化错误率/模型过拟合机率。就是限制神经元数量，因为过拟合的数据集是难以预测的。但同样，正则化的缺点是产生不自然图像，所以要以更好方式正则化产生更加可识别的可视效果。

## 201700150139 赵晓辉

**功能** ：描述神经网络每一层的内部构造。 

**可视化神经元做法** ：通过给每个图像x增加 $\alpha\cdot\partial ai(x)/\partial x$ 的大小使得神经元i有着较高的激活率。 

为了防止神经元错误识别图像，我们将其正则化。在此，为了产生更多可识别的图像，我们以更好的方式正则化以可视化每一个神经元。 

在此，优化正则化的方法为，使用4种不同的正则化方法，将这4中方法进行组合，生成更加可识别的样本。 

**可视化神经网络的所有层** ：可视化神经网络的所有层，是将反应神经网络不同层的图像抽取出来，然后通过几个不同的随机梯度下降方向，对每一层的每一个要素进行可视化。 

可视化结果表明，高层的神经网络因为结合了低层神经网络的特征而变得更加复杂。 

**深度可视化工具箱** ：Deep Visualization Toolbox可让您通过向DNN提供图像(或实时网络摄像头图像)，观察每个神经元的反应来探测DNN。并且可以选择单个神经元以查看该神经元可视化效果。 

## 201700210069 单宝迪

众所周知，神经网络的普及与人们对他的了解，是不成正比的。换句话说，人们对于神经网络内部的实现过程知之甚少。因此，通过深度可视化（*Deep Visualization*），我们可以对神经网络有更明确地认识。

为了更好的呈现结果，其构造了一个目标函数

$x^*=arg_xmax(a_i(x)-R_\theta(x))$

于此同时，为了寻找使式子最大的$x$，得到我们要的可视化结果$x^*$,通过下式来更新$x$:

$x\leftarrow r_\theta(x+\eta\frac{\partial a_i}{\partial x})$

即通过正则化更新x.
本文提出了四种正则化：
L2 decay:这个正则化的思想就是通过 来惩罚大的值，排除一些极端像素。这些极端像素，也可以叫做噪声点，这些点对可视化没有用。

Gaussian blur: 高斯模糊就是把某一点周围的像素色值按高斯曲线统计起来，采用数学上加权平均的计算方法得到这条曲线的色值，最后能够留下人物的轮廓，达到平滑的效果。这么做的思想是惩罚高频，这些高频引起高的激活值，但他们是既非现实的也非可理解的。

Clipping pixels with small norm:前两种发表方法是抑制高振幅高频信息，留下来的图像包涵很多非零像素。这些非零像素会识别多个目标，而我们希望只识别主要的那个目标。因此我们的做法是设置个阈值，将小于阈值的像素置为零

Clipping pixels with small contribution:计算出每个像素对激活值的贡献，即把像素x置为0 ，看激活值增加还是减少。即把第j个像素置为0。这中方法直观但效率特别低，实现起来非常慢，因此文中采用了一种近似的求解方法。梯度和x的内积，RGB三个通道相加，求绝对值，然后和阈值比较。


将它们组合在一起可比以前的方法生成更多可识别的基于优化的样本。最后得到一组优化的图像，
